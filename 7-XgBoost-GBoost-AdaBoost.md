# XGBoost, GB, AdaBoost Classifier and Regressor


## What are the Basic Assumption?
No such assumptions.

## Missing values
1. Adaboost can handle missing values
2. Xgboost and GBoost cannot handle missing values


## Advantages (AdaBoost)
1. Doesn't overfit
2. It has few parameters to tune.

## Advantages (Gradient and XGBoost)
1. It has a great performance.
2. It can solve complex non linear functions.
3. It is better in solving any kind of ML usecases.

## Disadvantages (Gradient and XGBoost)
- It requires some amount of parameter tuning

## Whether Feature Scaling is required?
No

## Impact of Outliers
Robust to Outliers in Gradient Boosting and XGBoost, Sensitive to outliers in AdaBoost.
